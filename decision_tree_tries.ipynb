{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e669c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2 3 4 5 6 7 8 9]\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.998528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22837\n",
      "[LightGBM] [Info] Number of data points in the train set: 15915263, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.774311\n",
      "[LightGBM] [Info] Start training from score -6.903321\n",
      "[LightGBM] [Info] Start training from score -8.009027\n",
      "[LightGBM] [Info] Start training from score -2.219580\n",
      "[LightGBM] [Info] Start training from score -1.833900\n",
      "[LightGBM] [Info] Start training from score -2.193644\n",
      "[LightGBM] [Info] Start training from score -2.086688\n",
      "[LightGBM] [Info] Start training from score -2.096901\n",
      "[LightGBM] [Info] Start training from score -2.391472\n",
      "[LightGBM] [Info] Start training from score -2.201109\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's multi_logloss: 1.60059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.62      0.52   1156814\n",
      "           1       0.38      0.40      0.39      6851\n",
      "           2       0.27      0.28      0.28      2268\n",
      "           3       0.39      0.57      0.46    741116\n",
      "           4       0.43      0.67      0.52   1089894\n",
      "           5       0.44      0.30      0.36    760588\n",
      "           6       0.48      0.45      0.46    846447\n",
      "           7       0.48      0.34      0.40    837847\n",
      "           8       0.60      0.25      0.35    624071\n",
      "           9       0.63      0.19      0.29    754932\n",
      "\n",
      "    accuracy                           0.45   6820828\n",
      "   macro avg       0.45      0.41      0.40   6820828\n",
      "weighted avg       0.48      0.45      0.43   6820828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Convert object columns to categorical\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Show class labels\n",
    "print(\"Unique classes:\", y.unique())\n",
    "\n",
    "# Train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Set up LightGBM model for multiclass\n",
    "num_classes = y.nunique()\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=num_classes,\n",
    "    max_depth=10,\n",
    "    n_estimators=1000\n",
    ")\n",
    "\n",
    "# Fit with early stopping callback\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)],\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd2739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.60      0.41   1156814\n",
      "           1       0.89      0.98      0.93      6851\n",
      "           2       0.99      1.00      0.99      2268\n",
      "           3       0.61      0.81      0.69    741116\n",
      "           4       0.31      0.57      0.40   1089894\n",
      "           5       0.28      0.15      0.19    760588\n",
      "           6       0.33      0.15      0.21    846447\n",
      "           7       0.30      0.29      0.29    837847\n",
      "           8       0.43      0.00      0.00    624071\n",
      "           9       0.30      0.02      0.03    754932\n",
      "\n",
      "    accuracy                           0.35   6820828\n",
      "   macro avg       0.48      0.46      0.42   6820828\n",
      "weighted avg       0.35      0.35      0.30   6820828\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.2542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Define preprocessing (one-hot encode categorical features)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)],\n",
    "    remainder=\"passthrough\"  # numeric columns stay as is\n",
    ")\n",
    "\n",
    "# Create LinearSVC pipeline\n",
    "svm_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LinearSVC(max_iter=10000, dual=False)  # dual=False is faster for large dense datasets\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0940cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import mode\n",
    "import faiss\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Feature / label split\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "# Fill missing values\n",
    "X = X.fillna(\"missing\")\n",
    "X[num_cols] = X[num_cols].fillna(0)\n",
    "\n",
    "# Hash categorical features to sparse matrix\n",
    "X_cat_dicts = X[cat_cols].astype(str).to_dict(orient=\"records\")\n",
    "hasher = FeatureHasher(n_features=1024, input_type='dict')\n",
    "X_cat_hashed_sparse = hasher.transform(X_cat_dicts)\n",
    "\n",
    "# Scale numerical features\n",
    "X_num_scaled = StandardScaler().fit_transform(X[num_cols]).astype(\"float32\")\n",
    "X_num_sparse = sparse.csr_matrix(X_num_scaled)\n",
    "\n",
    "# Combine features\n",
    "X_combined_sparse = hstack([X_cat_hashed_sparse, X_num_sparse]).tocsr()\n",
    "\n",
    "# Print estimated memory if made dense\n",
    "approx_gb = X_combined_sparse.shape[0] * X_combined_sparse.shape[1] * 4 / 1024**3\n",
    "print(f\"Estimated size if dense: {approx_gb:.2f} GB\")\n",
    "\n",
    "# Reduce dimensions to a manageable size\n",
    "svd = TruncatedSVD(n_components=128, random_state=42)\n",
    "X_reduced = svd.fit_transform(X_combined_sparse).astype(\"float32\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# FAISS\n",
    "d = X_train.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(X_train)\n",
    "\n",
    "# Search\n",
    "k = 5\n",
    "_, indices = index.search(X_test, k)\n",
    "\n",
    "# Predict\n",
    "y_pred = mode(y_train.to_numpy()[indices], axis=1).mode.flatten()\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"MCC:\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaa4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.49      0.59   1156814\n",
      "           1       0.33      0.98      0.50      6851\n",
      "           2       1.00      1.00      1.00      2268\n",
      "           3       0.29      0.93      0.44    741116\n",
      "           4       0.64      0.57      0.60   1089894\n",
      "           5       0.53      0.51      0.52    760588\n",
      "           6       0.64      0.48      0.55    846447\n",
      "           7       0.77      0.36      0.49    837847\n",
      "           8       0.58      0.52      0.55    624071\n",
      "           9       0.71      0.33      0.45    754932\n",
      "\n",
      "    accuracy                           0.52   6820828\n",
      "   macro avg       0.62      0.62      0.57   6820828\n",
      "weighted avg       0.62      0.52      0.53   6820828\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.4742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", make_pipeline(\n",
    "            SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        ), cat_cols),\n",
    "        (\"num\", SimpleImputer(strategy=\"mean\"), num_cols)\n",
    "    ]\n",
    ") \n",
    "\n",
    "# Random Forest model pipeline\n",
    "rf_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=100,           # Increase for better accuracy, decrease for speed\n",
    "        max_depth=20,               # Set to None for full growth\n",
    "        n_jobs=-1,                  # Use all CPU cores\n",
    "        class_weight=\"balanced\",   # Useful for imbalanced classes\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep                  float64\n",
      "cpm_id                   category\n",
      "vehicle_id               category\n",
      "detected_vehicle_id      category\n",
      "object_type              category\n",
      "detected_object_type     category\n",
      "label_detected              int64\n",
      "x_detected                float64\n",
      "y_detected                float64\n",
      "speed_detected            float64\n",
      "heading_detected          float64\n",
      "acceleration_detected     float64\n",
      "is_self                     int64\n",
      "sensor_id                category\n",
      "sensor_type              category\n",
      "sensor_range                int64\n",
      "weather_conditions       category\n",
      "visibility_range            int64\n",
      "detection_confidence      float64\n",
      "dtype: object\n",
      "timestep                 0\n",
      "cpm_id                   0\n",
      "vehicle_id               0\n",
      "detected_vehicle_id      0\n",
      "object_type              0\n",
      "detected_object_type     0\n",
      "label_detected           0\n",
      "x_detected               0\n",
      "y_detected               0\n",
      "speed_detected           0\n",
      "heading_detected         0\n",
      "acceleration_detected    0\n",
      "is_self                  0\n",
      "sensor_id                0\n",
      "sensor_type              0\n",
      "sensor_range             0\n",
      "weather_conditions       0\n",
      "visibility_range         0\n",
      "detection_confidence     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'CPM_truck43_586.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m HistGradientBoostingClassifier(\n\u001b[1;32m     33\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     34\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:361\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# time spent predicting X for gradient and hessians update\u001b[39;00m\n\u001b[1;32m    360\u001b[0m acc_prediction_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 361\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_DTYPE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_y(y)\n\u001b[1;32m    363\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/pandas/core/generic.py:1993\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: NpDtype \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 1993\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'CPM_truck43_586.0'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# ❗ Convert all object columns to pandas 'category' dtype BEFORE train-test split\n",
    "for col in X.select_dtypes(include=\"object\").columns:\n",
    "    X[col] = X[col].astype(\"category\")\n",
    "assert not any(X.dtypes == \"object\"), \"Object columns remain!\"\n",
    "\n",
    "print(X.dtypes) \n",
    "\n",
    "# Fill missing in categorical columns with a special label\n",
    "for col in X.select_dtypes(include=\"category\").columns:\n",
    "    X[col] = X[col].cat.add_categories([\"__missing__\"]).fillna(\"__missing__\")\n",
    "\n",
    "print(X.isna().sum())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Define model\n",
    "model = HistGradientBoostingClassifier(\n",
    "    max_iter=100,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Final dtypes before training:\\n\", X_train.dtypes)\n",
    "print(\"Sample row:\\n\", X_train.head(1).to_dict())\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeee3889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.00      0.00   1156814\n",
      "           1       0.09      0.69      0.16      6851\n",
      "           2       0.13      0.95      0.23      2268\n",
      "           3       0.26      0.42      0.32    741116\n",
      "           4       0.54      0.04      0.08   1089894\n",
      "           5       0.34      0.03      0.06    760588\n",
      "           6       0.52      0.03      0.06    846447\n",
      "           7       0.43      0.03      0.06    837847\n",
      "           8       0.11      0.87      0.19    624071\n",
      "           9       0.27      0.05      0.09    754932\n",
      "\n",
      "    accuracy                           0.15   6820828\n",
      "   macro avg       0.31      0.31      0.12   6820828\n",
      "weighted avg       0.39      0.15      0.09   6820828\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.0914\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Define preprocessing (OneHotEncoding)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Create SGDClassifier pipeline\n",
    "sgd_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    SGDClassifier(\n",
    "        loss=\"hinge\",        # Linear SVM\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        class_weight=\"balanced\",  # optional if class imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Fit\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Preprocessing with OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# RidgeClassifier pipeline\n",
    "ridge_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RidgeClassifier(\n",
    "        alpha=1.0,\n",
    "        class_weight=\"balanced\",  # good for imbalanced classes\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e462b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatihyuce/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/fatihyuce/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [05:56:20] WARNING: /croot/xgboost-split_1749630910898/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.72      0.58   1156814\n",
      "           1       0.82      0.93      0.87      6851\n",
      "           2       1.00      0.99      1.00      2268\n",
      "           3       0.41      0.65      0.50    741116\n",
      "           4       0.41      0.76      0.53   1089894\n",
      "           5       0.61      0.24      0.35    760588\n",
      "           6       0.55      0.47      0.51    846447\n",
      "           7       0.61      0.34      0.44    837847\n",
      "           8       0.69      0.24      0.36    624071\n",
      "           9       0.70      0.19      0.29    754932\n",
      "\n",
      "    accuracy                           0.48   6820828\n",
      "   macro avg       0.63      0.55      0.54   6820828\n",
      "weighted avg       0.55      0.48      0.46   6820828\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.4115\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Preprocessing: OneHotEncode categoricals\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# XGBoost classifier pipeline\n",
    "xgb_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    XGBClassifier(\n",
    "        objective=\"multi:softmax\",         # or \"multi:softprob\" if you want probabilities\n",
    "        num_class=len(y.unique()),\n",
    "        eval_metric=\"mlogloss\",\n",
    "        max_depth=10,\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        use_label_encoder=False,\n",
    "        tree_method=\"hist\",                # use \"gpu_hist\" if running on GPU\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8a0d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatihyuce/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Detect categorical columns (CatBoost accepts column names or indices)\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "X[cat_cols] = X[cat_cols].fillna(\"missing\").astype(str)  # Avoid NaNs + force string\n",
    "\n",
    "# Fill missing values in categorical columns with a string\n",
    "X[cat_cols] = X[cat_cols].fillna(\"missing\")\n",
    "# Optional: fill missing numeric values\n",
    "num_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "X[num_cols] = X[num_cols].fillna(0)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Define CatBoost model\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    learning_rate=0.1,\n",
    "    depth=10,\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='MultiClass',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    auto_class_weights=\"Balanced\"\n",
    ")\n",
    "\n",
    "# Train the model (using Pool to specify categorical features)\n",
    "catboost_model.fit(\n",
    "    Pool(X_train, y_train, cat_features=cat_cols),\n",
    "    eval_set=Pool(X_test, y_test, cat_features=cat_cols),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "y_pred = y_pred.flatten()  # Ensure it is 1D\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7706f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatihyuce/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fatihyuce/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fatihyuce/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.64      0.28   1156814\n",
      "           1       0.30      0.24      0.27      6851\n",
      "           2       1.00      0.92      0.96      2268\n",
      "           3       0.17      0.03      0.06    741116\n",
      "           4       0.17      0.27      0.21   1089894\n",
      "           5       0.00      0.00      0.00    760588\n",
      "           6       0.20      0.09      0.13    846447\n",
      "           7       0.21      0.10      0.14    837847\n",
      "           8       0.21      0.00      0.01    624071\n",
      "           9       0.00      0.00      0.00    754932\n",
      "\n",
      "    accuracy                           0.18   6820828\n",
      "   macro avg       0.25      0.23      0.21   6820828\n",
      "weighted avg       0.15      0.18      0.12   6820828\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Fill missing, then ordinal encode categoricals\n",
    "        (\"cat\", make_pipeline(\n",
    "            SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        ), cat_cols),\n",
    "        # Fill missing in numeric columns\n",
    "        (\"num\", SimpleImputer(strategy=\"mean\"), num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Final model pipeline\n",
    "nb_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    GaussianNB()\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit and evaluate\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TabNet\n",
    "#pip install pytorch-tabnet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Target variable\n",
    "y = df[\"cls_detected\"]\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(\"missing\")\n",
    "\n",
    "# Encode categorical columns and collect their indices\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "cat_idxs = [X.columns.get_loc(col) for col in cat_cols]\n",
    "cat_dims = []\n",
    "\n",
    "# LabelEncode categorical columns\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    cat_dims.append(len(le.classes_))\n",
    "\n",
    "# Encode target variable\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# TabNet model\n",
    "tabnet_model = TabNetClassifier(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=1,  # or use int or list like [3, 3, ...]\n",
    "    n_d=32,\n",
    "    n_a=32,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=1e-3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    seed=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "tabnet_model.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_name=[\"val\"],\n",
    "    eval_metric=[\"accuracy\"],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=16384,\n",
    "    virtual_batch_size=512\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = tabnet_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd901b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fastai\n",
    "#pip install fastai\n",
    "import pandas as pd\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Fill missing values\n",
    "df = df.fillna(\"missing\")\n",
    "\n",
    "# Define target and categorical/numeric columns\n",
    "target = 'cls_detected'\n",
    "cat_names = df.select_dtypes(include='object').columns.tolist()\n",
    "cont_names = df.select_dtypes(exclude='object').drop(columns=[target]).columns.tolist()\n",
    "\n",
    "# Split train/valid sets (random 70/30 split)\n",
    "splits = RandomSplitter(seed=42)(range_of(df))\n",
    "\n",
    "# Create DataLoaders\n",
    "to = TabularPandas(\n",
    "    df,\n",
    "    procs=[Categorify, FillMissing, Normalize],\n",
    "    cat_names=cat_names,\n",
    "    cont_names=cont_names,\n",
    "    y_names=target,\n",
    "    splits=splits\n",
    ")\n",
    "\n",
    "dls = to.dataloaders(bs=1024)\n",
    "\n",
    "# Build and train the model\n",
    "learn = tabular_learner(\n",
    "    dls,\n",
    "    metrics=[accuracy, RocAuc()],\n",
    "    layers=[200, 100],\n",
    "    emb_drop=0.1\n",
    ")\n",
    "\n",
    "learn.fit_one_cycle(5)\n",
    "\n",
    "# Evaluate\n",
    "preds, targs = learn.get_preds()\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.print_classification_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf10069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepctr/deepfm\n",
    "#pip install deepctr[pytorch]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from deepctr_torch.models import DeepFM  # or use WideDeep\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "import torch\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Target\n",
    "target = 'cls_detected'\n",
    "\n",
    "# Fill missing values\n",
    "df = df.fillna(\"missing\")\n",
    "\n",
    "# Encode categorical features\n",
    "cat_features = df.select_dtypes(include='object').columns.tolist()\n",
    "for col in cat_features:\n",
    "    df[col] = df[col].astype(str).astype(\"category\").cat.codes\n",
    "\n",
    "# Encode target\n",
    "df[target] = df[target].astype(\"category\").cat.codes\n",
    "\n",
    "# Identify feature types\n",
    "sparse_features = cat_features\n",
    "dense_features = [f for f in df.columns if f not in sparse_features + [target]]\n",
    "\n",
    "# Feature columns config\n",
    "fixlen_feature_columns = [\n",
    "    SparseFeat(feat, vocabulary_size=df[feat].nunique(), embedding_dim=4)\n",
    "    for feat in sparse_features\n",
    "] + [\n",
    "    DenseFeat(feat, 1,)\n",
    "    for feat in dense_features\n",
    "]\n",
    "\n",
    "# Get feature names\n",
    "feature_names = get_feature_names(fixlen_feature_columns)\n",
    "\n",
    "# Split train/test\n",
    "train, test = train_test_split(df, test_size=0.3, stratify=df[target], random_state=42)\n",
    "\n",
    "# Build model input\n",
    "train_model_input = {name: train[name].values for name in feature_names}\n",
    "test_model_input = {name: test[name].values for name in feature_names}\n",
    "\n",
    "# Labels\n",
    "y_train = train[target].values\n",
    "y_test = test[target].values\n",
    "\n",
    "# Define DeepFM (can replace with WideDeep or xDeepFM)\n",
    "model = DeepFM(\n",
    "    linear_feature_columns=fixlen_feature_columns,\n",
    "    dnn_feature_columns=fixlen_feature_columns,\n",
    "    task='multiclass',\n",
    "    l2_reg_embedding=1e-5,\n",
    "    dnn_hidden_units=(256, 128),\n",
    "    dnn_dropout=0.3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(train_model_input, y_train, batch_size=1024, epochs=10, verbose=2, validation_split=0.1)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(test_model_input, batch_size=1024)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "print(\"MCC:\", round(matthews_corrcoef(y_test, y_pred_labels), 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egcnoi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
