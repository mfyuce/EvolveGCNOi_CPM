{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e669c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2 3 4 5 6 7 8 9]\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.998528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22837\n",
      "[LightGBM] [Info] Number of data points in the train set: 15915263, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.774311\n",
      "[LightGBM] [Info] Start training from score -6.903321\n",
      "[LightGBM] [Info] Start training from score -8.009027\n",
      "[LightGBM] [Info] Start training from score -2.219580\n",
      "[LightGBM] [Info] Start training from score -1.833900\n",
      "[LightGBM] [Info] Start training from score -2.193644\n",
      "[LightGBM] [Info] Start training from score -2.086688\n",
      "[LightGBM] [Info] Start training from score -2.096901\n",
      "[LightGBM] [Info] Start training from score -2.391472\n",
      "[LightGBM] [Info] Start training from score -2.201109\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's multi_logloss: 1.60059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.62      0.52   1156814\n",
      "           1       0.38      0.40      0.39      6851\n",
      "           2       0.27      0.28      0.28      2268\n",
      "           3       0.39      0.57      0.46    741116\n",
      "           4       0.43      0.67      0.52   1089894\n",
      "           5       0.44      0.30      0.36    760588\n",
      "           6       0.48      0.45      0.46    846447\n",
      "           7       0.48      0.34      0.40    837847\n",
      "           8       0.60      0.25      0.35    624071\n",
      "           9       0.63      0.19      0.29    754932\n",
      "\n",
      "    accuracy                           0.45   6820828\n",
      "   macro avg       0.45      0.41      0.40   6820828\n",
      "weighted avg       0.48      0.45      0.43   6820828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Convert object columns to categorical\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Show class labels\n",
    "print(\"Unique classes:\", y.unique())\n",
    "\n",
    "# Train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Set up LightGBM model for multiclass\n",
    "num_classes = y.nunique()\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=num_classes,\n",
    "    max_depth=10,\n",
    "    n_estimators=1000\n",
    ")\n",
    "\n",
    "# Fit with early stopping callback\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)],\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd2739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.60      0.41   1156814\n",
      "           1       0.89      0.98      0.93      6851\n",
      "           2       0.99      1.00      0.99      2268\n",
      "           3       0.61      0.81      0.69    741116\n",
      "           4       0.31      0.57      0.40   1089894\n",
      "           5       0.28      0.15      0.19    760588\n",
      "           6       0.33      0.15      0.21    846447\n",
      "           7       0.30      0.29      0.29    837847\n",
      "           8       0.43      0.00      0.00    624071\n",
      "           9       0.30      0.02      0.03    754932\n",
      "\n",
      "    accuracy                           0.35   6820828\n",
      "   macro avg       0.48      0.46      0.42   6820828\n",
      "weighted avg       0.35      0.35      0.30   6820828\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.2542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Define preprocessing (one-hot encode categorical features)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)],\n",
    "    remainder=\"passthrough\"  # numeric columns stay as is\n",
    ")\n",
    "\n",
    "# Create LinearSVC pipeline\n",
    "svm_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LinearSVC(max_iter=10000, dual=False)  # dual=False is faster for large dense datasets\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0940cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import mode\n",
    "import faiss\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Feature / label split\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "# Fill missing values\n",
    "X = X.fillna(\"missing\")\n",
    "X[num_cols] = X[num_cols].fillna(0)\n",
    "\n",
    "# Hash categorical features to sparse matrix\n",
    "X_cat_dicts = X[cat_cols].astype(str).to_dict(orient=\"records\")\n",
    "hasher = FeatureHasher(n_features=1024, input_type='dict')\n",
    "X_cat_hashed_sparse = hasher.transform(X_cat_dicts)\n",
    "\n",
    "# Scale numerical features\n",
    "X_num_scaled = StandardScaler().fit_transform(X[num_cols]).astype(\"float32\")\n",
    "X_num_sparse = sparse.csr_matrix(X_num_scaled)\n",
    "\n",
    "# Combine features\n",
    "X_combined_sparse = hstack([X_cat_hashed_sparse, X_num_sparse]).tocsr()\n",
    "\n",
    "# Print estimated memory if made dense\n",
    "approx_gb = X_combined_sparse.shape[0] * X_combined_sparse.shape[1] * 4 / 1024**3\n",
    "print(f\"Estimated size if dense: {approx_gb:.2f} GB\")\n",
    "\n",
    "# Reduce dimensions to a manageable size\n",
    "svd = TruncatedSVD(n_components=128, random_state=42)\n",
    "X_reduced = svd.fit_transform(X_combined_sparse).astype(\"float32\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# FAISS\n",
    "d = X_train.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(X_train)\n",
    "\n",
    "# Search\n",
    "k = 5\n",
    "_, indices = index.search(X_test, k)\n",
    "\n",
    "# Predict\n",
    "y_pred = mode(y_train.to_numpy()[indices], axis=1).mode.flatten()\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"MCC:\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaa4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.49      0.59   1156814\n",
      "           1       0.33      0.98      0.50      6851\n",
      "           2       1.00      1.00      1.00      2268\n",
      "           3       0.29      0.93      0.44    741116\n",
      "           4       0.64      0.57      0.60   1089894\n",
      "           5       0.53      0.51      0.52    760588\n",
      "           6       0.64      0.48      0.55    846447\n",
      "           7       0.77      0.36      0.49    837847\n",
      "           8       0.58      0.52      0.55    624071\n",
      "           9       0.71      0.33      0.45    754932\n",
      "\n",
      "    accuracy                           0.52   6820828\n",
      "   macro avg       0.62      0.62      0.57   6820828\n",
      "weighted avg       0.62      0.52      0.53   6820828\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.4742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", make_pipeline(\n",
    "            SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        ), cat_cols),\n",
    "        (\"num\", SimpleImputer(strategy=\"mean\"), num_cols)\n",
    "    ]\n",
    ") \n",
    "\n",
    "# Random Forest model pipeline\n",
    "rf_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=100,           # Increase for better accuracy, decrease for speed\n",
    "        max_depth=20,               # Set to None for full growth\n",
    "        n_jobs=-1,                  # Use all CPU cores\n",
    "        class_weight=\"balanced\",   # Useful for imbalanced classes\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep                  float64\n",
      "cpm_id                   category\n",
      "vehicle_id               category\n",
      "detected_vehicle_id      category\n",
      "object_type              category\n",
      "detected_object_type     category\n",
      "label_detected              int64\n",
      "x_detected                float64\n",
      "y_detected                float64\n",
      "speed_detected            float64\n",
      "heading_detected          float64\n",
      "acceleration_detected     float64\n",
      "is_self                     int64\n",
      "sensor_id                category\n",
      "sensor_type              category\n",
      "sensor_range                int64\n",
      "weather_conditions       category\n",
      "visibility_range            int64\n",
      "detection_confidence      float64\n",
      "dtype: object\n",
      "timestep                 0\n",
      "cpm_id                   0\n",
      "vehicle_id               0\n",
      "detected_vehicle_id      0\n",
      "object_type              0\n",
      "detected_object_type     0\n",
      "label_detected           0\n",
      "x_detected               0\n",
      "y_detected               0\n",
      "speed_detected           0\n",
      "heading_detected         0\n",
      "acceleration_detected    0\n",
      "is_self                  0\n",
      "sensor_id                0\n",
      "sensor_type              0\n",
      "sensor_range             0\n",
      "weather_conditions       0\n",
      "visibility_range         0\n",
      "detection_confidence     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'CPM_truck43_586.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m HistGradientBoostingClassifier(\n\u001b[1;32m     33\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     34\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:361\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# time spent predicting X for gradient and hessians update\u001b[39;00m\n\u001b[1;32m    360\u001b[0m acc_prediction_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 361\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_DTYPE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_y(y)\n\u001b[1;32m    363\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/egcnoi_env/lib/python3.10/site-packages/pandas/core/generic.py:1993\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: NpDtype \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 1993\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'CPM_truck43_586.0'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# ❗ Convert all object columns to pandas 'category' dtype BEFORE train-test split\n",
    "for col in X.select_dtypes(include=\"object\").columns:\n",
    "    X[col] = X[col].astype(\"category\")\n",
    "assert not any(X.dtypes == \"object\"), \"Object columns remain!\"\n",
    "\n",
    "print(X.dtypes) \n",
    "\n",
    "# Fill missing in categorical columns with a special label\n",
    "for col in X.select_dtypes(include=\"category\").columns:\n",
    "    X[col] = X[col].cat.add_categories([\"__missing__\"]).fillna(\"__missing__\")\n",
    "\n",
    "print(X.isna().sum())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Define model\n",
    "model = HistGradientBoostingClassifier(\n",
    "    max_iter=100,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Final dtypes before training:\\n\", X_train.dtypes)\n",
    "print(\"Sample row:\\n\", X_train.head(1).to_dict())\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee3889",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable function object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     28\u001b[0m sgd_model \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[1;32m     29\u001b[0m     preprocessor,\n\u001b[1;32m     30\u001b[0m     SGDClassifier(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train/test split\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable function object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"burst_adma_with_cpm_multi_sensors_cls/burst_adma_with_cpm_multi_sensors999_cls_all.csv\")\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(\"cls_detected\", axis=1)\n",
    "y = df[\"cls_detected\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Define preprocessing (OneHotEncoding)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Create SGDClassifier pipeline\n",
    "sgd_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    SGDClassifier(\n",
    "        loss=\"hinge\",        # Linear SVM\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        class_weight=\"balanced\",  # optional if class imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred), 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egcnoi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
